{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8486786",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e571da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c632864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sending a GET request to the URL\n",
    "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b38d7ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the response\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2eb8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61d0089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list\n",
    "headers_article = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99656477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the headers in the content\n",
    "page_titles = soup.find_all(['h1','h2','h3','h4','h5','h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c13af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>, <h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>, <h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>, <h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>, <h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>, <h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>, <h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>, <h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>, <h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>, <h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>]\n"
     ]
    }
   ],
   "source": [
    "#displays all the header tags\n",
    "print(page_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95563267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through the contents and extracting all the headers\n",
    "for header in soup.find_all(['h1','h2','h3','h4','h5','h6']):\n",
    "    text = header.text.strip()\n",
    "    header_tag = header.name\n",
    "    headers_article.append((text, header_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8f6cdd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Text Header_tag\n",
      "0                      Main Page         h1\n",
      "1           Welcome to Wikipedia         h1\n",
      "2  From today's featured article         h2\n",
      "3               Did you know ...         h2\n",
      "4                    In the news         h2\n",
      "5                    On this day         h2\n",
      "6       Today's featured picture         h2\n",
      "7       Other areas of Wikipedia         h2\n",
      "8    Wikipedia's sister projects         h2\n",
      "9            Wikipedia languages         h2\n"
     ]
    }
   ],
   "source": [
    "#Creating a DataFrame from the extracted header data\n",
    "df = pd.DataFrame(headers_article, columns=[\"Text\", \"Header_tag\"])\n",
    "\n",
    "#display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba4698",
   "metadata": {},
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f56ca8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ab1c70b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://presidentofindia.nic.in/former-presidents\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "83f0e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d803a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list\n",
    "frm_presidents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a75698fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all the past presidents in the articles on the page\n",
    "president = soup.find_all('div',class_=\"desc-sec\")\n",
    "\n",
    "#display the content result\n",
    "#president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "817a5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name                     Term\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    }
   ],
   "source": [
    "#looping through the contents and extract all the past presidents\n",
    "for president in soup.find_all('div',class_=\"desc-sec\"):\n",
    "    name = president.find(\"h3\").text.strip()\n",
    "    term = president.find(\"h5\").text.strip()\n",
    "    frm_presidents.append((name, term))\n",
    "\n",
    "#Creating a DataFrame from the extracted data\n",
    "df = pd.DataFrame(frm_presidents, columns=[\"Name\", \"Term\"])\n",
    "\n",
    "#display result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579876d",
   "metadata": {},
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28c07f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36bc6124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sending a GET request to the URL\n",
    "\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e4457e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "727611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list\n",
    "teamodi_men = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9715365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the contents and extract all the teams, record of matches, points and rating\n",
    "for team in soup.select(\"tbody tr\"):\n",
    "    name = team.select(\"td\")[1].text.strip()\n",
    "    matches = team.select(\"td\")[2].text.strip()\n",
    "    points = team.select(\"td\")[3].text.strip()\n",
    "    rating = team.select(\"td\")[4].text.strip()\n",
    "    teamodi_men.append((name, matches, points, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4a3826b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      23  2,714    118\n",
      "1     Pakistan\\nPAK      20  2,316    116\n",
      "2        India\\nIND      36  4,081    113\n",
      "3   New Zealand\\nNZ      27  2,806    104\n",
      "4      England\\nENG      24  2,426    101\n",
      "5  South Africa\\nSA      19  1,910    101\n",
      "6   Bangladesh\\nBAN      28  2,661     95\n",
      "7  Afghanistan\\nAFG      16  1,404     88\n",
      "8     Sri Lanka\\nSL      32  2,794     87\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "#create dataframe having columns Team, matches, points and rating\n",
    "df = pd.DataFrame(teamodi_men, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "\n",
    "#displays the first top 10 teams\n",
    "df = df.head(10)  \n",
    "\n",
    "#print result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8245d1",
   "metadata": {},
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c826b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0851046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8bd1b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "856374df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Player Team Rating\n",
      "0             Babar Azam  PAK    886\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2           Fakhar Zaman  PAK    755\n",
      "3            Imam-ul-Haq  PAK    745\n",
      "4           Shubman Gill  IND    743\n",
      "5           Harry Tector  IRE    726\n",
      "6           David Warner  AUS    726\n",
      "7        Quinton de Kock   SA    718\n",
      "8            Virat Kohli  IND    705\n",
      "9            Steve Smith  AUS    702\n"
     ]
    }
   ],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "bat_players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "# Find all the top ten batting players\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "#Get the top 10 players\n",
    "for row in rows[1:11]: \n",
    "    columns = row.find_all(\"td\")\n",
    "    \n",
    "    #Specifying the columns where the data for each variable can be found    \n",
    "    player_name = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "    \n",
    "    #Adding players name, team and ratings to the list\n",
    "    bat_players.append(player_name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "    \n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\"Player\": bat_players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "#Creating a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#displays the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6fcb2",
   "metadata": {},
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e1171784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7a8f14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7b4d8327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1    Mitchell Starc  AUS    686\n",
      "2       Rashid Khan  AFG    682\n",
      "3    Mohammed Siraj  IND    670\n",
      "4        Matt Henry   NZ    667\n",
      "5  Mujeeb Ur Rahman  AFG    661\n",
      "6       Trent Boult   NZ    660\n",
      "7        Adam Zampa  AUS    652\n",
      "8    Shaheen Afridi  PAK    630\n",
      "9     Kuldeep Yadav  IND    622\n"
     ]
    }
   ],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "bowl_players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "#Find all the top ten bowling players\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "#Get the top 10 players\n",
    "for row in rows[1:11]:  \n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    #Specifying the columns where the data for each variable can be found \n",
    "    player_name = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "    \n",
    "   #Adding players name, team and ratings to the list\n",
    "    bowl_players.append(player_name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "    \n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Player\": bowl_players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "#Creating a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#displays the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daaaaef",
   "metadata": {},
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dced3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9db3ddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f5312ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0334ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "team_women = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "086c154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the contents and extract all the teams, record of matches, points and rating\n",
    "for team in soup.select(\"tbody tr\"):\n",
    "    name = team.select(\"td\")[1].text.strip()\n",
    "    matches = team.select(\"td\")[2].text.strip()\n",
    "    points = team.select(\"td\")[3].text.strip()\n",
    "    rating = team.select(\"td\")[4].text.strip()\n",
    "    team_women.append((name, matches, points, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "72a9a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      26  4,290    165\n",
      "1      England\\nENG      31  3,875    125\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      30  3,039    101\n",
      "4   New Zealand\\nNZ      28  2,688     96\n",
      "5   West Indies\\nWI      29  2,743     95\n",
      "6   Bangladesh\\nBAN      17  1,284     76\n",
      "7     Sri Lanka\\nSL      12    820     68\n",
      "8     Thailand\\nTHA      13    883     68\n",
      "9     Pakistan\\nPAK      27  1,678     62\n"
     ]
    }
   ],
   "source": [
    "#create dataframe\n",
    "df = pd.DataFrame(team_women, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "\n",
    "# get the top 10 teams and display the result\n",
    "df = df.head(10)  \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45d7a6",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e2153b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Sending a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8a7f43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#Display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ada9f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Player Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    803\n",
      "1   Chamari Athapaththu   SL    758\n",
      "2           Beth Mooney  AUS    751\n",
      "3       Laura Wolvaardt   SA    732\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    702\n",
      "6      Harmanpreet Kaur  IND    694\n",
      "7          Ellyse Perry  AUS    686\n",
      "8           Meg Lanning  AUS    682\n",
      "9       Stafanie Taylor   WI    618\n"
     ]
    }
   ],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "bat_players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "#Find all the top ten batting players\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "#Get the top 10 players\n",
    "for row in rows[1:11]:\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    #Specifying the columns where the data for each variable can be found \n",
    "    player_name = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "    \n",
    "    #Adding players name, team and ratings to the list\n",
    "    bat_players.append(player_name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "    \n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Player\": bat_players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "#Creating a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#display result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140efad",
   "metadata": {},
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d56dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Send a GET request to the URL\n",
    "response = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "26e1c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b8d76ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Player Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    421\n",
      "1      Ashleigh Gardner  AUS    389\n",
      "2       Hayley Matthews   WI    382\n",
      "3        Marizanne Kapp   SA    349\n",
      "4          Ellyse Perry  AUS    329\n",
      "5           Amelia Kerr   NZ    328\n",
      "6         Deepti Sharma  IND    312\n",
      "7         Jess Jonassen  AUS    241\n",
      "8         Sophie Devine   NZ    233\n",
      "9              Nida Dar  PAK    232\n"
     ]
    }
   ],
   "source": [
    "#Create empty list for players, teams, rating\n",
    "rounder_players = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "#Find all the top ten batting players\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "#Get the top 10 players\n",
    "for row in rows[1:11]:\n",
    "    columns = row.find_all(\"td\")\n",
    " \n",
    "  #Specifying the columns where the data for each variable can be found \n",
    "    player_name = columns[1].text.strip()\n",
    "    team = columns[2].text.strip()\n",
    "    rating = columns[3].text.strip()\n",
    "\n",
    "    #Adding players name, team and ratings to the list\n",
    "    rounder_players.append(player_name)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "    \n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Player\": rounder_players,\n",
    "    \"Team\": teams,\n",
    "    \"Rating\": ratings\n",
    "}\n",
    "#Creating a DataFrame from the dictionar\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#display result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d234146",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "589a8d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Send a GET request to the URL\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5215eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4b4813e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline             Time  \\\n",
      "0   Why 'career choices' is the No. 1 conflict amo...      5 Hours Ago   \n",
      "1   Cruise will reduce robotaxi fleet by 50% in Sa...      6 Hours Ago   \n",
      "2   Can American-made weapons like F-16s turn the ...      7 Hours Ago   \n",
      "3   Harvard gut doctor avoids these 4 foods that c...      9 Hours Ago   \n",
      "4   The clash of sustainability and AI is creating...      9 Hours Ago   \n",
      "5   On tap next week: 2 housing reports and 2 Inve...      9 Hours Ago   \n",
      "6          Top 10 best European cities for retirement      9 Hours Ago   \n",
      "7   Google's plan to purge inactive accounts isn't...     10 Hours Ago   \n",
      "8   The No. 1 best state to retire in the U.S.—it'...     10 Hours Ago   \n",
      "9   Mark Cuban passed on an Uber investment that c...     10 Hours Ago   \n",
      "10  Believing these 5 Social Security myths may re...     10 Hours Ago   \n",
      "11  Here's why Aldi is looking to the Southern U.S...     11 Hours Ago   \n",
      "12  Here's how the Huy Fong Foods sriracha shortag...     11 Hours Ago   \n",
      "13  This ETF is soaring in August as market swoon ...     11 Hours Ago   \n",
      "14  Morgan Stanley is among the most oversold in t...     11 Hours Ago   \n",
      "15  Buy these stocks with upside as market fears i...     11 Hours Ago   \n",
      "16  Palo Alto shares rise on earnings beat, after ...  August 18, 2023   \n",
      "17  Wall Street awaits hotly anticipated Nvidia ea...  August 18, 2023   \n",
      "18  WeWork plunges another 11% after announcing re...  August 18, 2023   \n",
      "19  The iPhone 15 could get one of the biggest upg...  August 18, 2023   \n",
      "20  Coral bleaching event in Florida is 'just the ...  August 18, 2023   \n",
      "21  Bitcoin is giving a bearish signal. Here’s wha...  August 18, 2023   \n",
      "22  Earnings show shoppers will spend money for va...  August 18, 2023   \n",
      "23  Nvidia, key Powell speech to take center stage...  August 18, 2023   \n",
      "24        My HomePod is now a very expensive doorstop  August 18, 2023   \n",
      "25  3 trends are dividing restaurant companies int...  August 18, 2023   \n",
      "26  How hurricanes may affect the 2024 Social Secu...  August 18, 2023   \n",
      "27  Rosenblatt names its top picks to play the ‘ag...  August 18, 2023   \n",
      "28  It may be tough for Apple to outperform from h...  August 18, 2023   \n",
      "29  'Blue Beetle' tries to take down 'Barbie' in a...  August 18, 2023   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/08/19/why-career-cho...  \n",
      "1   https://www.cnbc.com/2023/08/19/cruise-will-re...  \n",
      "2   https://www.cnbc.com/2023/08/19/can-expensive-...  \n",
      "3   https://www.cnbc.com/2023/08/19/harvard-gut-do...  \n",
      "4   https://www.cnbc.com/2023/08/19/the-clash-of-s...  \n",
      "5   https://www.cnbc.com/2023/08/19/on-tap-next-we...  \n",
      "6   https://www.cnbc.com/2023/08/19/best-countries...  \n",
      "7   https://www.cnbc.com/2023/08/19/google-faces-c...  \n",
      "8   https://www.cnbc.com/2023/08/19/best-us-states...  \n",
      "9   https://www.cnbc.com/2023/08/19/mark-cuban-pas...  \n",
      "10  https://www.cnbc.com/2023/08/19/social-securit...  \n",
      "11  https://www.cnbc.com/2023/08/19/aldi-winn-dixi...  \n",
      "12  https://www.cnbc.com/2023/08/19/how-did-the-hu...  \n",
      "13  https://www.cnbc.com/2023/08/19/this-etf-is-so...  \n",
      "14  https://www.cnbc.com/2023/08/19/morgan-stanley...  \n",
      "15  https://www.cnbc.com/2023/08/19/goldman-picks-...  \n",
      "16  https://www.cnbc.com/2023/08/18/palo-alto-netw...  \n",
      "17  https://www.cnbc.com/2023/08/18/wall-street-pr...  \n",
      "18  https://www.cnbc.com/2023/08/18/wework-plunges...  \n",
      "19  https://www.cnbc.com/2023/08/18/iphone-15-usb-...  \n",
      "20  https://www.cnbc.com/2023/08/18/noaa-florida-c...  \n",
      "21  https://www.cnbc.com/2023/08/18/bitcoin-is-giv...  \n",
      "22  https://www.cnbc.com/2023/08/18/retail-earning...  \n",
      "23  https://www.cnbc.com/2023/08/18/nvidia-powell-...  \n",
      "24  https://www.cnbc.com/2023/08/18/apple-wont-rep...  \n",
      "25  https://www.cnbc.com/2023/08/18/mcdonalds-chip...  \n",
      "26  https://www.cnbc.com/2023/08/18/social-securit...  \n",
      "27  https://www.cnbc.com/2023/08/18/rosenblatt-nam...  \n",
      "28  https://www.cnbc.com/2023/08/18/it-may-be-toug...  \n",
      "29  https://www.cnbc.com/2023/08/18/barbie-vs-blue...  \n"
     ]
    }
   ],
   "source": [
    "#Create empty list for headlines, times, news link\n",
    "headlines = []\n",
    "times = []\n",
    "news_links = []\n",
    "\n",
    "#Finding all the news articles on the web page\n",
    "news_articles = soup.find_all(\"div\", class_=\"LatestNews-container\")\n",
    "\n",
    "#Looping through each news and extract the relevant data\n",
    "for article in news_articles:\n",
    "    #Extracting headlines\n",
    "    headline = article.find(\"a\", class_=\"LatestNews-headline\").text.strip()\n",
    "    headlines.append(headline)\n",
    "\n",
    "    #Extracting time\n",
    "    time = article.find(\"time\").text.strip()\n",
    "    times.append(time)\n",
    "\n",
    "    #Extracting the link of the news\n",
    "    news_link = article.find(\"a\", class_=\"LatestNews-headline\")[\"href\"]\n",
    "    news_links.append(news_link)\n",
    "\n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Headline\": headlines,\n",
    "    \"Time\": times,\n",
    "    \"News Link\": news_links\n",
    "}\n",
    "#Create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Display the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d00eb8",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame\n",
    "# i) Paper Title\n",
    "# ii) Authors\n",
    "# iii) Published Date\n",
    "# iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e341e45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Send a GET request to the URL\n",
    "response = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4e183b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ea707c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all news articles on the web page\n",
    "article_items = soup.find_all(\"li\", class_=\"sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs\")\n",
    "#article_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "35435fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "#Creating empty Lists to store extracted data\n",
    "paper_titles = []\n",
    "authors_list = []\n",
    "published_dates = []\n",
    "paper_urls = []\n",
    "\n",
    "#Iterating through the article content to extract article details\n",
    "for item in article_items:\n",
    "    # Extracting title of the article\n",
    "    title = item.find(\"h2\", class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\").get_text(strip=True)\n",
    "    paper_titles.append(title)\n",
    "    \n",
    "    # Extracting the authors\n",
    "    authors = item.find(\"span\",{'class': \"sc-1w3fpd7-0 dnCnAO\"}).get_text(strip=True)\n",
    "    authors_list.append(authors)\n",
    "    \n",
    "    # Extracting the published date\n",
    "    published_date = item.find(\"span\", class_=\"sc-1thf9ly-2 dvggWt\").get_text(strip=True)\n",
    "    published_dates.append(published_date)\n",
    "    \n",
    "    # Extracting the paper URL\n",
    "    paper_url = item.find(\"a\", class_=\"sc-5smygv-0 fIXTHm\")[\"href\"]\n",
    "    paper_urls.append(paper_url)\n",
    "\n",
    "#Creating a dictionary from the extracted data\n",
    "data = {\n",
    "    \"Paper Title\": paper_titles,\n",
    "    \"Authors\": authors_list,\n",
    "    \"Published Date\": published_dates,\n",
    "    \"Paper URL\": paper_urls\n",
    "}\n",
    "#Creating a DataFrame from the dictionar\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397253e8",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location\n",
    "# iv) Ratings\n",
    "# v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "839a58e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Send a GET request to the URL\n",
    "response = requests.get('https://www.dineout.co.in/delhi-restaurants')\n",
    "\n",
    "#display response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9b070c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Parsing the content of HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#display content\n",
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dac8a7",
   "metadata": {},
   "source": [
    "# Hi,\n",
    "# I could not access the website's contents to extract information for this question. See the response received below.\n",
    "# img alt=\"access denied\" src=\"https://im1.dineout.co.in/images/uploads/misc/2020/Aug/5/image_2.png\" title=\"Access Denied\"/> Food not found! p>Looks like you are not in our service area but don't worry, we take uniting foodies with food very seriously and will come to your region soon!/p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c1e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
